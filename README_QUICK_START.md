# BigYellowData - Quick Start Guide

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis
- Docker & Docker Compose
- Java 11+ (pour sbt/Scala local)
- 8GB RAM minimum

### Commande unique pour tout lancer

```bash
# Lancer TOUS les exercices (Ex01 â†’ Ex02 â†’ Ex03)
./setup_and_run.sh all

# OU lancer un exercice spÃ©cifique
./setup_and_run.sh ex01   # Data Retrieval vers MinIO
./setup_and_run.sh ex02   # Data Ingestion & Cleaning
./setup_and_run.sh ex03   # SQL Data Warehouse Setup
```

Le script `setup_and_run.sh` fait TOUT automatiquement :
- âœ… DÃ©marre Docker (Spark, MinIO, PostgreSQL, pgAdmin)
- âœ… CrÃ©e le bucket MinIO
- âœ… Upload les fichiers nÃ©cessaires (taxi_zone_lookup.csv)
- âœ… Lance les exercices Spark
- âœ… Initialise le Data Warehouse PostgreSQL

### ğŸ“Š AccÃ¨s aux Services

| Service | URL | Credentials |
|---------|-----|-------------|
| **Spark Master UI** | http://localhost:8081 | - |
| **MinIO Console** | http://localhost:9001 | Voir `.env` (MINIO_ROOT_USER / MINIO_ROOT_PASSWORD) |
| **pgAdmin** | http://localhost:5050 | Email: `admin@admin.com` / Pass: `admin` |
| **PostgreSQL** | localhost:5432 | User: `user_dw` / Pass: `password_dw` / DB: `nyc_data_warehouse` |

### ğŸ—‚ï¸ Structure du Projet

```
BigYellowData/
â”œâ”€â”€ setup_and_run.sh          # ğŸ¯ SCRIPT PRINCIPAL (utilisez celui-ci!)
â”œâ”€â”€ run_spark_docker.sh       # Script bas-niveau (utilisÃ© par setup_and_run.sh)
â”œâ”€â”€ docker-compose.yml        # Configuration Docker
â”œâ”€â”€ data/raw/                 # DonnÃ©es brutes (parquet files)
â”œâ”€â”€ ex01_data_retrieval/      # Exercice 1: Upload vers MinIO
â”œâ”€â”€ ex02_data_ingestion/      # Exercice 2: Nettoyage des donnÃ©es
â”œâ”€â”€ ex03_sql_table_creation/  # Exercice 3: Data Warehouse SQL
â”‚   â”œâ”€â”€ creation.sql          # CrÃ©ation du schÃ©ma (constellation)
â”‚   â”œâ”€â”€ insertion.sql         # DonnÃ©es de rÃ©fÃ©rence
â”‚   â”œâ”€â”€ aggregation.sql       # Tables agrÃ©gÃ©es
â”‚   â””â”€â”€ README.md             # Documentation dÃ©taillÃ©e du DWH
â”œâ”€â”€ ex04_dashboard/           # Exercice 4: Streamlit Dashboard
â””â”€â”€ ex05_ml_prediction_service/ # Exercice 5: Machine Learning
```

### ğŸ”„ Workflow Complet

```
1. Data Lake (MinIO)
   â†“
2. Data Cleaning (Spark Ex02)
   â†“
3. Data Warehouse (PostgreSQL Ex03)
   â†“
4. Dashboard (Streamlit Ex04)
   â†“
5. ML Model (Python Ex05)
```

### ğŸ› ï¸ Commandes Utiles

```bash
# Voir les logs d'un conteneur
docker logs -f spark-master
docker logs -f postgres-dw

# VÃ©rifier les donnÃ©es dans MinIO
docker exec minio mc ls -r myminio/nyctaxiproject/

# Se connecter Ã  PostgreSQL
docker exec -it postgres-dw psql -U user_dw -d nyc_data_warehouse

# RequÃªte SQL rapide
docker exec -it postgres-dw psql -U user_dw -d nyc_data_warehouse -c "SELECT COUNT(*) FROM dw.fact_trip;"

# Stopper tout
docker-compose down

# Nettoyer les volumes (âš ï¸ Supprime toutes les donnÃ©es!)
docker-compose down -v
```

### ğŸ“ˆ VÃ©rification Post-ExÃ©cution

AprÃ¨s avoir lancÃ© `./setup_and_run.sh all`, vÃ©rifiez:

**MinIO**:
```bash
docker exec minio mc ls myminio/nyctaxiproject/
# Devrait afficher:
# - nyc_raw/ (depuis Ex01)
# - dwh/yellow_taxi_refined/ (depuis Ex02)
# - taxi_zone_lookup.csv
```

**PostgreSQL**:
```bash
docker exec postgres-dw psql -U user_dw -d nyc_data_warehouse -c "
SET search_path TO dw;
SELECT
  'dim_vendor' as table_name, COUNT(*) as rows FROM dim_vendor
UNION ALL
SELECT 'dim_location', COUNT(*) FROM dim_location
UNION ALL
SELECT 'dim_date', COUNT(*) FROM dim_date
UNION ALL
SELECT 'fact_trip', COUNT(*) FROM fact_trip;
"
```

RÃ©sultat attendu:
- dim_vendor: 4 rows
- dim_location: 265 rows
- dim_date: 3,650 rows
- fact_trip: ~5-10M rows (dÃ©pend de Ex02)

### ğŸ› Troubleshooting

**ProblÃ¨me**: "Port already in use"
```bash
# VÃ©rifier les ports occupÃ©s
docker ps
# Stopper les conteneurs
docker-compose down
```

**ProblÃ¨me**: "Out of memory"
```bash
# Augmenter la mÃ©moire Docker (Docker Desktop Settings)
# Minimum 8GB recommandÃ©
```

**ProblÃ¨me**: "Permission denied"
```bash
chmod +x setup_and_run.sh
chmod +x run_spark_docker.sh
```

**ProblÃ¨me**: Compilation SBT Ã©choue
```bash
cd ex02_data_ingestion
sbt clean
sbt compile
```

### ğŸ“ Notes Importantes

1. **PremiÃ¨re exÃ©cution**: Plus lente (tÃ©lÃ©chargement des images Docker)
2. **DonnÃ©es**: Les 3 mois de donnÃ©es = ~200MB
3. **DurÃ©e**:
   - Ex01: ~2-3 min
   - Ex02: ~5-10 min (dÃ©pend des donnÃ©es)
   - Ex03: ~30 secondes
4. **Persistence**: Les donnÃ©es restent mÃªme aprÃ¨s `docker-compose down` (sauf si `-v`)

### ğŸ“ Pour aller plus loin

- **Ex03 Architecture**: Voir [ex03_sql_table_creation/README.md](ex03_sql_table_creation/README.md)
- **ModÃ¨le Constellation**: Justification et diagrammes dans le README Ex03
- **Dashboard (Ex04)**: Ã€ implÃ©menter avec Streamlit
- **ML (Ex05)**: PrÃ©diction de `total_amount` depuis les parquet MinIO

### âœ… Checklist Avant de Push

- [ ] `./setup_and_run.sh all` fonctionne sans erreur
- [ ] Tous les conteneurs sont UP: `docker ps`
- [ ] MinIO contient les donnÃ©es: `docker exec minio mc ls -r myminio/nyctaxiproject/`
- [ ] PostgreSQL a les tables: Voir "VÃ©rification Post-ExÃ©cution"
- [ ] Les logs Spark ne montrent pas d'erreurs critiques

---

**DerniÃ¨re mise Ã  jour**: 2026-01-14
**Auteur**: Nadir (Ã©quipe BigYellowData)
